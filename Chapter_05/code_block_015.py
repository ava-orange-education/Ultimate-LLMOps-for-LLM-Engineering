# This ensures LLM gets most relevant context within token limits